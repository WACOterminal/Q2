workflow_id: "wf_test_driven_development"
original_prompt: "Refactor the specified file and ensure all tests pass."
shared_context:
  source_file: "path/to/your/source_file.py"
  test_file_path: "path/to/your/tests/test_file.py"
  test_command: "pytest path/to/your/tests/ --cov={{ shared_context.source_file }} --cov-report=term-missing"
  refactor_request: "Please refactor the 'calculate_total' function for clarity."
  function_to_test: "calculate_total"

tasks:
  - task_id: "read_source_code"
    type: "task"
    agent_personality: "default"
    prompt: "Read the full content of the file at '{{ shared_context.source_file }}'."
    dependencies: []

  - task_id: "attempt_first_refactor"
    type: "task"
    agent_personality: "default"
    prompt: |
      Based on the refactor request: "{{ shared_context.refactor_request }}",
      Modify the following code:
      ---
      {{ tasks.read_source_code.result }}
      ---
      Your output for this task MUST be ONLY the complete, new source code for the file.
    dependencies:
      - "read_source_code"
  
  - task_id: "test_and_fix_loop"
    type: "loop"
    dependencies: ["attempt_first_refactor"]
    # The loop continues as long as the tests fail and we haven't hit the iteration limit.
    condition: "{{ 'Exit Code: 0' not in tasks.get('run_tests', {}).result|default('') or (tasks.get('run_static_analysis', {}).result and tasks.get('run_static_analysis', {}).result.status == 'issues_found') }}"
    max_iterations: 5
    tasks:
      - task_id: "save_code_for_test"
        type: "task"
        agent_personality: "default"
        # On the first iteration, use the result of the initial refactor.
        # On subsequent iterations, use the result of the last fix attempt.
        prompt: "Write the following content to '{{ shared_context.source_file }}':\n\n{{ tasks.get('attempt_fix', tasks.attempt_first_refactor).result }}"
        dependencies: []

      - task_id: "run_static_analysis"
        type: "task"
        agent_personality: "default"
        prompt: "Run the linter on the file '{{ shared_context.source_file }}'."
        dependencies: ["save_code_for_test"]

      - task_id: "run_tests"
        type: "task"
        agent_personality: "default"
        prompt: "Execute the command: '{{ shared_context.test_command }}'."
        dependencies: ["run_static_analysis"]

      - task_id: "check_failure"
        type: "conditional"
        dependencies: ["run_tests", "run_static_analysis"]
        branches:
          # This branch only runs if the tests failed, to prepare for the next loop iteration.
          - condition: >
              {{ 'Exit Code: 0' not in tasks.run_tests.result or
                 (tasks.run_static_analysis.result and tasks.run_static_analysis.result.status == 'issues_found') }}
            tasks:
              - task_id: "attempt_fix"
                type: "task"
                agent_personality: "default"
                prompt: |
                  The tests failed or the linter found issues.
                  
                  Linter output:
                  ---
                  {{ tasks.run_static_analysis.result }}
                  ---
                  
                  Test output:
                  ---
                  {{ tasks.run_tests.result }}
                  ---
                  Based on this feedback, provide a corrected version of the source code.
                  Your last attempt was:
                  ---
                  {{ tasks.save_code_for_test.prompt }}
                  ---
                  Your output for this task MUST be ONLY the complete, new, corrected source code for the file.
                dependencies: []

  - task_id: "run_tests_with_coverage"
    type: "task"
    agent_personality: "default"
    prompt: "Execute the command: '{{ shared_context.test_command }}'."
    dependencies: ["test_and_fix_loop"]

  - task_id: "generate_new_tests"
    type: "task"
    agent_personality: "default"
    prompt: "Generate a new test for the function '{{ shared_context.function_to_test }}' in the file '{{ shared_context.source_file }}'."
    dependencies: ["run_tests_with_coverage"]

  - task_id: "save_new_test"
    type: "task"
    agent_personality: "default"
    prompt: "Append the following content to '{{ shared_context.test_file_path }}':\n\n{{ tasks.generate_new_tests.result }}"
    dependencies: ["generate_new_tests"]

  - task_id: "create_git_branch"
    type: "task"
    agent_personality: "default"
    prompt: "Create a new branch named 'agent-fix/{{ workflow.workflow_id }}' in the repository '{{ shared_context.repo }}'."
    dependencies: ["save_new_test"]

  - task_id: "commit_changes"
    type: "task"
    agent_personality: "default"
    prompt: |
      Commit the successfully tested code to the new branch.
      Repo: '{{ shared_context.repo }}'
      Branch: 'agent-fix/{{ workflow.workflow_id }}'
      File Path: '{{ shared_context.source_file }}'
      Commit Message: 'Fix: Automated refactor by AgentQ for goal {{ workflow.original_prompt }}'
      Content:
      ---
      {{ tasks.get('attempt_fix', tasks.attempt_first_refactor).result }}
      ---
    dependencies: ["create_git_branch"]

  - task_id: "create_pull_request"
    type: "task"
    agent_personality: "default"
    prompt: |
      Create a pull request for the changes.
      Repo: '{{ shared_context.repo }}'
      Head Branch: 'agent-fix/{{ workflow.workflow_id }}'
      Base Branch: 'main'
      Title: 'Automated Refactor: {{ shared_context.refactor_request }}'
      Body: |
        This pull request was automatically generated by AgentQ to address the goal: "{{ workflow.original_prompt }}". All tests have passed.
        
        **Static Analysis Report:**
        ```
        {{ tasks.run_static_analysis.result }}
        ```

        **Coverage Report:**
        ```
        {{ tasks.run_tests_with_coverage.result }}
        ```
    dependencies: ["commit_changes"] 